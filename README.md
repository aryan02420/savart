## Preview

https://savart.vercel.app/

## Architecture

The app is built using Next js app router which supports server components.

Redis is used on the back end to store the scraped results. Improves load times significantly if cache hit. see the [docker compose file](docker-compose.dev.yml) for starting a redis server and its api. The environment variables for development are already defined in [`.env.development`](.env.development). for production deployment, configure upstash in the vercel storage dashboard. vercel will automatically generate and use the necessary environment variables.

The app has two screens.

1. home
2. stock details

home is client side, since it only contains an interactive element. see [`app/page.tsx`](src/app/page.tsx).

stock details is server rendered. scraping is performed on the backend and only minimal html is sent to the client vis RSC. see [`app/stock/[id]/page.tsx`](src/app/stock/[id]/page.tsx)

`components/ui`: this folder is for components generated by shadcn.

`components/views`: similar to router views. contains component code for rendering entire pages. It has been put here since we are already doing a lot of fetching / processing on the server side component [`app/stock/[id]/page.tsx`](src/app/stock/[id]/page.tsx).

`lib`: random functions and one time utilities.

`app/api/**/route.ts`: Next js standard file. export functions from this file to generate api endpoints.

`app/api/**/rpc.ts`: Not a standard next js file. Contains binding code to be used by the client. equivalent to using trpc or generating fetchers from open api schema.

using tanstack-query for data fetching on the frontend.

using happy-dom for scrapping on the backend. happy-dom is pure javascript implementation of the playwright/browser/dom api. it supports most of the features of playwright and since the scraped page is mostly static, it works well for our use case. it is also simpler to use and deploy compared to playwright or other headless browser solutions.


## Setup

Tools required to build and run the project:

```bash
$ node --version
v22.14.0

$ pnpm --version
10.6.3

$ docker --version
Docker version 27.5.1-ce, build 4c9b3b011ae4

$ docker-compose --version
docker-compose version 1.29.2, build 5becea4c
```

Install dependencies

```bash
$ pnpm install

$ docker-compose -f docker-compose.dev.yml pull
```

Start the app

```bash
$ docker-compose -f docker-compose.dev.yml up

$ pnpm dev
```

Shut down the app

```bash
$ docker-compose -f docker-compose.dev.yml up
```

For production build

```bash
$ yarn build

$ cp .env.{development,production} # Or set these directly in you shell based on the values provided by vercel dashboard.

$ yarn start
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

## Differences from requirements

- Testing has not been integrated. Since majority of logic is dependent on 3rd party, for which testing does not make sense. Instead it would be preferable to rely on observability. Zod and browser assertions can be used to ensure the data we scrape is of the proper shape and log it accordingly.
- Search does not support ISIN. You can instead search based on company name or stock symbol. This should be easy to swap with some other API. The lookup record can also be generated based on data provided by NSDL (I could not find the one which also lists NSE/BSE symbols which are used by screener.in).
- Next js has been used in order to simplify deployment. Can be substituted for Vite based SPA, but I did not want to write a separate backend.

## Enhancements

- Better caching strategy. It would probably be ideal if we could display stale data while scraping for new one on the side.
- Better error handling and observability. Right now all errors are wrapped into a generic error with no context of what actually went wrong. On actual apps, I would prefer finer grained control over errors and also better logging for the same.
- setup linting and formatting.
- Design lol.